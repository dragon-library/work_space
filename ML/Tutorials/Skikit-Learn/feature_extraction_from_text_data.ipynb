{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn - Feature Extraction from Text Data\n",
    "====\n",
    "\n",
    "- https://coderzcolumn.com/tutorials/machine-learning/feature-extraction-from-text-data-using-scikit-learn-sklearn\n",
    "\n",
    "\n",
    "## Feature Extraction From Text Data\n",
    "\n",
    "All of the machine learning libraries expect input in the form of floats and that also fixed length/dimensions. But in real life, we face data in different forms like text, images, audio, video, etc. We need to find a way to represent these forms of data as floats to be able to train learning algorithms based on them. In this tutorial, we'll be discussing how to convert free form text which can be of variable length to an array of floats (called feature extraction generally).\n",
    "\n",
    "### Bag Of Words\n",
    "\n",
    "We'll start with a simple method for representation of text data called a bag of words.\n",
    "\n",
    "Here, we'll be assuming data has come to us as a single string for each instance(spam mail, book, new, etc.) of data. We'll split each instance to a list of tokens based on white space and then lowercase each word. We'll repeat this process for each of our instances in the dataset. At the end of the process, we'll have quite a big vocabulary of words from all instances.\n",
    "\n",
    "Now looking at each of our samples we can tell how often it appears in vocabulary. We'll represent our string as a single vector of length the same as that of vocabulary and words from that string will be marked  `1s`  & all other entries will be  `0s`  in that vector. We'll repeat the process for each instance of data.\n",
    "\n",
    "At the end of the process, we'll end up with an array of size  `(number_of_instance/samples x vocabulary_size)`  which will be quite a sparse array because the dictionary contains all possible words and each sentence will have few words from it.\n",
    "\n",
    "It's called bag-of-words because the order of words is lost totally.\n",
    "\n",
    "![](https://storage.googleapis.com/coderzcolumn/static/tutorials/machine_learning/bag_of_words.svg)\n",
    "\n",
    "We'll start by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:48:00.728207Z",
     "start_time": "2020-11-23T14:47:51.693095Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample Dataset\n",
    "Below we have created a sample dataset of 3 strings which we'll be using for an explanation of our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:48:35.072710Z",
     "start_time": "2020-11-23T14:48:35.061673Z"
    }
   },
   "outputs": [],
   "source": [
    "X = ['Welcome to coderzcolumn. We will help you learn python',\n",
    "     'Lets start our day by learning something new',\n",
    "    'Learn from tutorials, learn from blogs. Keep learning till life ends. Its a long journey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model & Fitting to Data\n",
    "We'll be using a simple CounteVectorizer provided by scikit-learn for converting our list of strings to a list of tokens based on vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:49:21.249004Z",
     "start_time": "2020-11-23T14:49:21.118729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transforming Data From Text to Floats\n",
    "\n",
    "Once we have an initialized model and trained it with train data, we can then transform data to floats using the  `transform()`  method. We can also use the  `fit_transform()`  method available in an object to perform fitting and transforming data in one step if we want to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:50:44.463751Z",
     "start_time": "2020-11-23T14:50:44.447760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 27),\n",
       " <3x27 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 29 stored elements in Compressed Sparse Row format>,\n",
       " scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_input = vectorizer.transform(X)\n",
    "vectorized_input.shape, vectorized_input, type(vectorized_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note that transformed input is returning sparse scipy array which is stored in CSR(Compressed Sparse Row) format. One can easily convert such array to numpy array and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:51:03.633994Z",
     "start_time": "2020-11-23T14:51:03.613998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "          1, 0, 1, 1, 1, 1],\n",
       "         [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "          0, 1, 0, 0, 0, 0]], dtype=int64),\n",
       " array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 1, 1, 1, 1],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0]], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_input.todense(), vectorized_input.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:51:48.612961Z",
     "start_time": "2020-11-23T14:51:48.594961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 24, 'to': 21, 'coderzcolumn': 2, 'we': 23, 'will': 25, 'help': 6, 'you': 26, 'learn': 10, 'python': 17, 'lets': 12, 'start': 19, 'our': 16, 'day': 3, 'by': 1, 'learning': 11, 'something': 18, 'new': 15, 'from': 5, 'tutorials': 22, 'blogs': 0, 'keep': 9, 'till': 20, 'life': 13, 'ends': 4, 'its': 7, 'long': 14, 'journey': 8}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:51:59.769229Z",
     "start_time": "2020-11-23T14:51:59.753235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogs', 'by', 'coderzcolumn', 'day', 'ends', 'from', 'help', 'its', 'journey', 'keep', 'learn', 'learning', 'lets', 'life', 'long', 'new', 'our', 'python', 'something', 'start', 'till', 'to', 'tutorials', 'we', 'welcome', 'will', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can transform sparse array back to the original list of strings using the `inverse_transform()` method but we'll have lost our order of words in original sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:52:10.132321Z",
     "start_time": "2020-11-23T14:52:10.116362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['coderzcolumn', 'help', 'learn', 'python', 'to', 'we', 'welcome',\n",
      "       'will', 'you'], dtype='<U12'), array(['by', 'day', 'learning', 'lets', 'new', 'our', 'something',\n",
      "       'start'], dtype='<U12'), array(['blogs', 'ends', 'from', 'its', 'journey', 'keep', 'learn',\n",
      "       'learning', 'life', 'long', 'till', 'tutorials'], dtype='<U12')]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.inverse_transform(vectorized_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Of Other Important Parameters\n",
    "\n",
    "We'll below list down other important parameters available in the  `CountVectorizer`  model which can help us with various purposes when extracting futures from text data.\n",
    "\n",
    "**input**  - It accepts one of string values from list ['content', 'filename','file'].  `content`  expects list of  `strings/bytes`  as input.  `filename`  expects list of filename as input.`file`  expects list of file objects as input.  `default=content`\n",
    "\n",
    "**encoding**  - If the list of bytes or files opened in binary mode are given as input then this parameter is used to decode data.  `default=utf-8`\n",
    "\n",
    "**decode_error**  - It accepts string from list ['strict', 'ignore', 'replace'].  `strict`  will fail vectorizer if there is error when decoding byte sequence.`ignore`  will ignore characters where errors occur while decoding.  `replace`  will replace with suitable matching character if error occurs while decoding.`default=strict`.\n",
    "\n",
    "**preprocessor**  - It accepts  `callable`  or  `None`  as value. We can create our own preprocessor function which takes as input string and performs preprocessing according to our need. We can add lemmatization, stemming, etc.  `default=None`\n",
    "\n",
    "**tokenizer**  - It accepts  `callable`  or  `None`  as value. We can define our own function which will split words according to our needs.It's only useful when  `analyzer=word`  is set.  `default=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:53:35.468883Z",
     "start_time": "2020-11-23T14:53:35.449839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary {'welcome': 25, 'to': 22, 'coderzcolumn': 3, 'we': 24, 'will': 26, 'help': 7, 'you': 27, 'learn': 11, 'python': 18, 'lets': 13, 'start': 20, 'our': 17, 'day': 4, 'by': 2, 'learning': 12, 'something': 19, 'new': 16, 'from': 6, 'tutorials': 23, 'blogs': 1, 'keep': 10, 'till': 21, 'life': 14, 'ends': 5, 'its': 8, 'a': 0, 'long': 15, 'journey': 9}\n"
     ]
    }
   ],
   "source": [
    "def user_defined_preprocessor(sample):\n",
    "    \"\"\"\n",
    "    sample: It returns to one sample of data.\n",
    "    returns: It returns string with special characters removed.\n",
    "    It returns list of words with only english characters seprated by single white space.\n",
    "    \"\"\"\n",
    "    return ' '.join(re.findall(r'\\w+', sample)) ## \\w captures [a-zA-Z0-9] chracters in data.\n",
    "\n",
    "def user_defined_tokenizer(sample):\n",
    "    \"\"\"\n",
    "    sample: It returns to one sample of data.\n",
    "    returns: It first lowers each chracter in string and then split them by single white space.\n",
    "    It then returns list of words.\n",
    "    \"\"\"\n",
    "    return sample.lower().split(' ')\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=user_defined_preprocessor, tokenizer=user_defined_tokenizer)\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary', vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can access preprocessor and tokenizer using `build_preprocessor()` and `build_tokenizer()` methods and `preprocessor` and `tokenizer` property of CountVectorizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:53:47.001948Z",
     "start_time": "2020-11-23T14:53:46.981945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function user_defined_preprocessor in module __main__:\n",
      "\n",
      "user_defined_preprocessor(sample)\n",
      "    sample: It returns to one sample of data.\n",
      "    returns: It returns string with special characters removed.\n",
      "    It returns list of words with only english characters seprated by single white space.\n",
      "\n",
      "None\n",
      "\n",
      "Help on function user_defined_tokenizer in module __main__:\n",
      "\n",
      "user_defined_tokenizer(sample)\n",
      "    sample: It returns to one sample of data.\n",
      "    returns: It first lowers each chracter in string and then split them by single white space.\n",
      "    It then returns list of words.\n",
      "\n",
      "None\n",
      "\n",
      "Preprocessor :  <function user_defined_preprocessor at 0x0000023A8E01DE18>\n",
      "\n",
      "Tokenizer :  <function user_defined_tokenizer at 0x0000023A8E01DF28>\n"
     ]
    }
   ],
   "source": [
    "print(help(vectorizer.build_preprocessor()))\n",
    "print()\n",
    "print(help(vectorizer.build_tokenizer()))\n",
    "print()\n",
    "print('Preprocessor : ', vectorizer.preprocessor)\n",
    "print()\n",
    "print('Tokenizer : ',vectorizer.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stop_words**  - It accepts string  `english`,  `list of words`  or  `None`  as value. It removes these words when performing tokenization hence it won't be available in final vocabulary. It's only applied when  `analyzer=word`.  `default=None`\n",
    "\n",
    "**token_pattern**  - It refers to tokenization pattern which will decide what can be defined as one token(word). It's only applied when  `analyzer=word`.  `default='(?u)\\\\b\\\\w\\\\w+\\\\b'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:55:04.705759Z",
     "start_time": "2020-11-23T14:55:04.674237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  {'welcome': 20, 'coderzcolumn': 1, 'will': 21, 'help': 5, 'learn': 8, 'python': 15, 'lets': 10, 'start': 17, 'our': 14, 'day': 2, 'learning': 9, 'something': 16, 'new': 13, 'from': 4, 'tutorials': 19, 'blogs': 0, 'keep': 7, 'till': 18, 'life': 11, 'ends': 3, 'long': 12, 'journey': 6}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=['we','you', 'it','its','to','a','an', 'the', 'by'])\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:55:16.525966Z",
     "start_time": "2020-11-23T14:55:16.507970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  {'welcome': 16, 'coderzcolumn': 1, 'help': 4, 'learn': 6, 'python': 12, 'lets': 8, 'start': 13, 'day': 2, 'learning': 7, 'new': 11, 'tutorials': 15, 'blogs': 0, 'till': 14, 'life': 9, 'ends': 3, 'long': 10, 'journey': 5}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:55:30.014216Z",
     "start_time": "2020-11-23T14:55:30.001216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  {'welcome': 24, 'to': 21, 'coderzcolumn': 2, 'we': 23, 'will': 25, 'help': 6, 'you': 26, 'learn': 10, 'python': 17, 'lets': 12, 'start': 19, 'our': 16, 'day': 3, 'by': 1, 'learning': 11, 'something': 18, 'new': 15, 'from': 5, 'tutorials': 22, 'blogs': 0, 'keep': 9, 'till': 20, 'life': 13, 'ends': 4, 'its': 7, 'long': 14, 'journey': 8}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make a note above how vocabulary is created with different use of  `stop_words`  values.\n",
    "\n",
    "**ngram_range**  - It accepts tuple of (min_n, max_n) which refers to the minimum and maximum values to be considered for n-grams. It's explained further below in tutorial in-depth.  `default=(1,1)`\n",
    "\n",
    "**analyzer**  - It accepts string from list ['word', 'char', 'char_wb'] as value. It decides what should be considered as one token( a word of a character).`default=word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:56:42.123262Z",
     "start_time": "2020-11-23T14:56:42.111175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  {'welcome coderzcolumn': 29, 'coderzcolumn help': 2, 'help learn': 8, 'learn python': 12, 'welcome coderzcolumn help': 30, 'coderzcolumn help learn': 3, 'help learn python': 9, 'lets start': 18, 'start day': 23, 'day learning': 4, 'learning new': 15, 'lets start day': 19, 'start day learning': 24, 'day learning new': 5, 'learn tutorials': 13, 'tutorials learn': 27, 'learn blogs': 10, 'blogs learning': 0, 'learning till': 16, 'till life': 25, 'life ends': 20, 'ends long': 6, 'long journey': 22, 'learn tutorials learn': 14, 'tutorials learn blogs': 28, 'learn blogs learning': 11, 'blogs learning till': 1, 'learning till life': 17, 'till life ends': 26, 'life ends long': 21, 'ends long journey': 7}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3), analyzer='word', stop_words='english')\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make a note above that we have considered only 2-words and 3-words as token while removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:57:28.138828Z",
     "start_time": "2020-11-23T14:57:28.128828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  [('welc', 295), ('elco', 97), ('lcom', 150), ('come', 78), ('ome ', 212), ('me t', 176), ('e to', 88), (' to ', 39), ('to c', 273), ('o co', 200), (' cod', 6), ('code', 74), ('oder', 202), ('derz', 82), ('erzc', 105), ('rzco', 248), ('zcol', 307), ('colu', 76), ('olum', 206), ('lumn', 170)]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(4, 5), analyzer='char', stop_words='english')\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', list(vectorizer.vocabulary_.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make a note above that we have considered only 4-characters and 5-characters as token while removing stop words. We are printing only 20 token to prevent output from flooding.\n",
    "\n",
    "**max_futures**  - It accepts  `int`  or  `None`  as value. If an integer is provided then only that many top tokens according to token-frequency will be considered across the corpus. If  `vocabulary`  parameter described below has been given then this parameter is ignored.  `default=None`\n",
    "\n",
    "**vocabulary**  - It accepts mapping(dict) or iterable as value. Mapping should be a dictionary with the key as token and value as indices. For iterable, it should be a list of (token, index) values.`default=None`\n",
    "\n",
    "**Note:**  Document-frequency represents a total number of the document that contains term token/term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:58:13.407260Z",
     "start_time": "2020-11-23T14:58:13.396270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  [('welcome', 9), ('to', 6), ('we', 8), ('learn', 2), ('start', 4), ('learning', 3), ('from', 1), ('tutorials', 7), ('blogs', 0), ('till', 5)]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10)\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', list(vectorizer.vocabulary_.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:58:26.139600Z",
     "start_time": "2020-11-23T14:58:26.128599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  [('welcome', 0), ('tutorials', 1), ('blogs', 2)]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(vocabulary={'welcome':0,'tutorials':1, 'blogs':2})\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', list(vectorizer.vocabulary_.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**min_df**  - It accepts float value in range [0.0, 1.0]. It ignores all tokens whose document-frequency is lower than given value.  `default=1`\n",
    "\n",
    "**max_df**  - It accepts float value in range [0.0, 1.0]. It ignores all tokens whose document-frequency is higher than given value.  `default=1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T14:59:13.221440Z",
     "start_time": "2020-11-23T14:59:13.207407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  [('welcome', 24), ('to', 21), ('coderzcolumn', 2), ('we', 23), ('will', 25), ('help', 6), ('you', 26), ('learn', 10), ('python', 17), ('lets', 12), ('start', 19), ('our', 16), ('day', 3), ('by', 1), ('learning', 11), ('something', 18), ('new', 15), ('from', 5), ('tutorials', 22), ('blogs', 0)]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.25, max_df=0.75)\n",
    "transformed_X = vectorizer.fit_transform(X)\n",
    "print('Vocabulary : ', list(vectorizer.vocabulary_.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf encoding\n",
    "\n",
    "tf-idf (term frequency-inverse document frequency) is a type of transformation applied to bag-of-words tokens. It's kind of scaling which can help complete training fast.\n",
    "\n",
    "The main idea behind scaling is that down weight words which occur in many documents because that kind of words will have less influence on natural processing tasks like document classification. It puts more emphasis on words that are less occurring giving them more weight than frequently occurring.\n",
    "\n",
    "We'll below explain step by step of getting tf-idf though scikit-learn has direct implementation for it as well.\n",
    "\n",
    "**Raw Term Frequency - tf(t,d):**  We already explained above raw term frequency and scikit-learn implementation  `CountVectorizer`  to get it.\n",
    "\n",
    "**Normalized Term Frequency:**  Raw term frequency is normalized using l2-normalization which involves dividing normal term frequency  vv  by its vector's length  ||v||||v||  (Euclidean Norm).\n",
    "\n",
    "vnorm=v||v||2=v(∑ni=1vi)1/2vnorm=v||v||2=v(∑i=1nvi)1/2\n",
    "\n",
    "**document frequency - df(d,t):**  It represents a total number of the document that contains term t.\n",
    "\n",
    "**inverse document frequency - idf(t):**  Formula for idf is given below based on document frequency.\n",
    "\n",
    "idf(t)=lognddf(d,t)+1idf(t)=log⁡nddf(d,t)+1\n",
    "\n",
    "**smooth_idf:**  Scikit-learn transformers have an attribute called  `smooth_idf`  which transforms  `idf`  formula mentioned above to below one.\n",
    "\n",
    "idf(t)=log1+nd1+df(d,t)+1idf(t)=log⁡1+nd1+df(d,t)+1\n",
    "\n",
    "**tf-idf:**  FInal formula based on above terms for tf-idf is given below\n",
    "\n",
    "tf−idf(t,d)=tf(t,d)∗idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:02:24.066828Z",
     "start_time": "2020-11-23T15:02:24.044785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Term Frequency of 3rd sample : \n",
      " [0.23570226 0.         0.         0.         0.23570226 0.47140452\n",
      " 0.         0.23570226 0.23570226 0.23570226 0.47140452 0.23570226\n",
      " 0.         0.23570226 0.23570226 0.         0.         0.\n",
      " 0.         0.         0.23570226 0.         0.23570226 0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "tf = vectorized_input.toarray()\n",
    "normalized_tf = tf[2] / np.sqrt(np.sum(tf[2]**2))\n",
    "print('Normalized Term Frequency of 3rd sample : \\n',normalized_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:02:38.541074Z",
     "start_time": "2020-11-23T15:02:38.519033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Term Frequency of 3rd sample : \n",
      " [0.23570226 0.         0.         0.         0.23570226 0.47140452\n",
      " 0.         0.23570226 0.23570226 0.23570226 0.47140452 0.23570226\n",
      " 0.         0.23570226 0.23570226 0.         0.         0.\n",
      " 0.         0.         0.23570226 0.         0.23570226 0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=False, norm='l2', smooth_idf=False)\n",
    "tf_normalized = tfidf.fit_transform(tf).toarray()\n",
    "print('Normalized Term Frequency of 3rd sample : \\n', tf_normalized[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:03:05.755437Z",
     "start_time": "2020-11-23T15:03:05.737436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf of \"welcome\" :  2.09861228866811\n",
      "idf of \"learn\" :  4.216395324324493\n"
     ]
    }
   ],
   "source": [
    "n_docs = len(X)\n",
    "tf_welcome = 1\n",
    "df_welcome = 1\n",
    "inverse_df_welcome = (np.log(n_docs / df_welcome) + 1)\n",
    "print('idf of \"welcome\" : ',tf_welcome * inverse_df_welcome)\n",
    "\n",
    "tf_learn = 3\n",
    "df_learn = 2\n",
    "inverse_df_learn = (np.log(n_docs / df_learn) + 1)\n",
    "print('idf of \"learn\" : ', tf_learn * inverse_df_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:03:28.978041Z",
     "start_time": "2020-11-23T15:03:28.964046Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(norm=None,smooth_idf=False,use_idf=True)\n",
    "tf_idf = tfidf.fit_transform(tf).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:03:40.119031Z",
     "start_time": "2020-11-23T15:03:40.103369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       1.40546511, 1.40546511, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:03:50.402670Z",
     "start_time": "2020-11-23T15:03:50.394667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 0.        , 0.        , 0.        , 2.09861229,\n",
       "       4.19722458, 0.        , 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.81093022, 1.40546511, 0.        , 2.09861229, 2.09861229,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       2.09861229, 0.        , 2.09861229, 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use the  `TfidfVectorizer`  class of scikit-learn for generating tf-idfs.\n",
    "\n",
    "**Note:**  Please make a note that  `TfidfTransformer`  works on term frequency array generated through  `CountVectorizer`  and  `TfidfVectorizer`  works directly on the original list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:04:05.913985Z",
     "start_time": "2020-11-23T15:04:05.898990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 0.        , 0.        , 0.        , 2.09861229,\n",
       "       4.19722458, 0.        , 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.81093022, 1.40546511, 0.        , 2.09861229, 2.09861229,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       2.09861229, 0.        , 2.09861229, 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(norm=None,smooth_idf=False,use_idf=True)\n",
    "tf_idf = tfidf_vect.fit_transform(X).toarray()\n",
    "tf_idf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:04:53.257052Z",
     "start_time": "2020-11-23T15:04:53.241026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf of \"welcome\" :  1.6931471805599454\n",
      "idf of \"learn\" :  3.8630462173553424\n"
     ]
    }
   ],
   "source": [
    "n_docs = len(X)\n",
    "tf_welcome = 1\n",
    "df_welcome = 1\n",
    "inverse_df_welcome = (np.log( (1+n_docs) / (1+df_welcome)) + 1)\n",
    "print('idf of \"welcome\" : ',tf_welcome * inverse_df_welcome)\n",
    "\n",
    "tf_learn = 3\n",
    "df_learn = 2\n",
    "inverse_df_learn = (np.log((1+n_docs) / (1+df_learn)) + 1)\n",
    "print('idf of \"learn\" : ', tf_learn * inverse_df_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:05:08.744731Z",
     "start_time": "2020-11-23T15:05:08.728767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69314718, 0.        , 0.        , 0.        , 1.69314718,\n",
       "       3.38629436, 0.        , 1.69314718, 1.69314718, 1.69314718,\n",
       "       2.57536414, 1.28768207, 0.        , 1.69314718, 1.69314718,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.69314718, 0.        , 1.69314718, 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(norm=None) ## Tfidf with no normalization. It'll be using idf and smoothing of idf though.\n",
    "tf_idf = tfidf_vect.fit_transform(X).toarray()\n",
    "tf_idf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:05:20.295013Z",
     "start_time": "2020-11-23T15:05:20.273012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25084807, 0.        , 0.        , 0.        , 0.25084807,\n",
       "       0.50169613, 0.        , 0.25084807, 0.25084807, 0.25084807,\n",
       "       0.38155284, 0.19077642, 0.        , 0.25084807, 0.25084807,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25084807, 0.        , 0.25084807, 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer() ## tfidf with l2 normalization, using idf and smoothing idf as well\n",
    "tf_idf = tfidf_vect.fit_transform(X).toarray()\n",
    "tf_idf[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Of Other Important Parameters\n",
    "\n",
    "`TfidfVectorizer`  has most of the parameter the same as that of  `Countvectorizer`  which we have explained above in-depth. One can try the parameter values explained above with  `TfidfVectorizer`  as well to check results. Parameters that were specific to  `TfidfVectorizer`  have been already explained above with examples.\n",
    "\n",
    "## Bigrams and N-Grams\n",
    "\n",
    "Till now we have discussed only one-word tokens(1-gram - unigram) and totally discarded order of words. But this might not be always right as we might need to consider the order in some scenarios (like \"not\" can invert the meaning of the sentence).\n",
    "\n",
    "A simple way to consider some order of words is to use n-grams. N-Grams does not look at single words but all pairs of possible neighbors.\n",
    "\n",
    "2-grams can consist of all 2 words neighboring pairs with an overlap of 1 word. 3-grams can consist of all 3 words neighboring pairs with an overlap of 2 words.\n",
    "\n",
    "-   Sample Text : \"Lets learn something new today\"\n",
    "-   1-gram : \"lets\", \"learn\", \"something\", \"new\", \"today\"\n",
    "-   2-gram : \"lets learn\", \"learn something\", \"something new\", \"new today\"\n",
    "-   3-gram : \"lets learn something\", \"learn something new\", \"something new today\"\n",
    "\n",
    "Deciding \"n\" to be used in n-gram is dependent on the application and can be used as one hyperparameter of the algorithm to be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:06:24.647037Z",
     "start_time": "2020-11-23T15:06:24.625694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(2, 2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2)) ## N-gram with min lenght of 2 and max length of 2\n",
    "bigram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:06:45.403318Z",
     "start_time": "2020-11-23T15:06:45.389279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogs keep', 'by learning', 'coderzcolumn we', 'day by', 'ends its', 'from blogs', 'from tutorials', 'help you', 'its long', 'keep learning', 'learn from', 'learn python', 'learning something', 'learning till', 'lets start', 'life ends', 'long journey', 'our day', 'something new', 'start our', 'till life', 'to coderzcolumn', 'tutorials learn', 'we will', 'welcome to', 'will help', 'you learn']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:04.245394Z",
     "start_time": "2020-11-23T15:07:04.215403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:13.950436Z",
     "start_time": "2020-11-23T15:07:13.930228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome to': 24, 'to coderzcolumn': 21, 'coderzcolumn we': 2, 'we will': 23, 'will help': 25, 'help you': 7, 'you learn': 26, 'learn python': 11, 'lets start': 14, 'start our': 19, 'our day': 17, 'day by': 3, 'by learning': 1, 'learning something': 12, 'something new': 18, 'learn from': 10, 'from tutorials': 6, 'tutorials learn': 22, 'from blogs': 5, 'blogs keep': 0, 'keep learning': 9, 'learning till': 13, 'till life': 20, 'life ends': 15, 'ends its': 4, 'its long': 8, 'long journey': 16}\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:30.136033Z",
     "start_time": "2020-11-23T15:07:30.112026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "gram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:07:47.768596Z",
     "start_time": "2020-11-23T15:07:47.762598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogs', 'blogs keep', 'by', 'by learning', 'coderzcolumn', 'coderzcolumn we', 'day', 'day by', 'ends', 'ends its', 'from', 'from blogs', 'from tutorials', 'help', 'help you', 'its', 'its long', 'journey', 'keep', 'keep learning', 'learn', 'learn from', 'learn python', 'learning', 'learning something', 'learning till', 'lets', 'lets start', 'life', 'life ends', 'long', 'long journey', 'new', 'our', 'our day', 'python', 'something', 'something new', 'start', 'start our', 'till', 'till life', 'to', 'to coderzcolumn', 'tutorials', 'tutorials learn', 'we', 'we will', 'welcome', 'welcome to', 'will', 'will help', 'you', 'you learn']\n"
     ]
    }
   ],
   "source": [
    "print(gram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:02.549367Z",
     "start_time": "2020-11-23T15:08:02.529328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:14.543068Z",
     "start_time": "2020-11-23T15:08:14.528024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 48, 'to': 42, 'coderzcolumn': 4, 'we': 46, 'will': 50, 'help': 13, 'you': 52, 'learn': 20, 'python': 35, 'welcome to': 49, 'to coderzcolumn': 43, 'coderzcolumn we': 5, 'we will': 47, 'will help': 51, 'help you': 14, 'you learn': 53, 'learn python': 22, 'lets': 26, 'start': 38, 'our': 33, 'day': 6, 'by': 2, 'learning': 23, 'something': 36, 'new': 32, 'lets start': 27, 'start our': 39, 'our day': 34, 'day by': 7, 'by learning': 3, 'learning something': 24, 'something new': 37, 'from': 10, 'tutorials': 44, 'blogs': 0, 'keep': 18, 'till': 40, 'life': 28, 'ends': 8, 'its': 15, 'long': 30, 'journey': 17, 'learn from': 21, 'from tutorials': 12, 'tutorials learn': 45, 'from blogs': 11, 'blogs keep': 1, 'keep learning': 19, 'learning till': 25, 'till life': 41, 'life ends': 29, 'ends its': 9, 'its long': 16, 'long journey': 31}\n"
     ]
    }
   ],
   "source": [
    "print(gram_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character N-Grams\n",
    "\n",
    "Sometimes we want to create tokens of a list of characters of a particular length. Character N-Grams are generally used in language identification.\n",
    "\n",
    "We can use  `analyzer=\"char\"`  for generating character n-grams with  `CountVectorizer`  as we had described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:09:01.045426Z",
     "start_time": "2020-11-23T15:09:01.034431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(2, 2))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer=\"char\")\n",
    "char_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:09:13.587105Z",
     "start_time": "2020-11-23T15:09:13.576107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a', ' b', ' c', ' d', ' e', ' f', ' h', ' i', ' j', ' k', ' l', ' n', ' o', ' p', ' s', ' t', ' w', ' y', ', ', '. ', 'a ', 'al', 'ar', 'ay', 'bl', 'by', 'co', 'da', 'de', 'ds', 'e ', 'ea', 'ee', 'el', 'en', 'ep', 'er', 'et', 'ew', 'ey', 'fe', 'fr', 'g ', 'gs', 'he', 'hi', 'ho', 'ia', 'if', 'il', 'in', 'it', 'jo', 'ke', 'l ', 'lc', 'le', 'li', 'll', 'lo', 'lp', 'ls', 'lu', 'm ', 'me', 'mn', 'n ', 'n.', 'nd', 'ne', 'ng', 'ni', 'o ', 'od', 'og', 'ol', 'om', 'on', 'or', 'ou', 'p ', 'py', 'r ', 'ri', 'rn', 'ro', 'rt', 'rz', 's ', 's,', 's.', 'so', 'st', 't ', 'ta', 'th', 'ti', 'to', 'ts', 'tu', 'u ', 'um', 'ur', 'ut', 'we', 'wi', 'y ', 'yo', 'yt', 'zc']\n"
     ]
    }
   ],
   "source": [
    "print(char_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection Case Study\n",
    "\n",
    "We'll perform an SMS Spam classification task from the UCI ML data library. It'll help us explain the whole process of text feature extraction, feature selection, training model, evaluating the model and visualizing results.\n",
    "\n",
    "## Flow of Whole Process\n",
    "\n",
    "![](https://storage.googleapis.com/coderzcolumn/static/tutorials/machine_learning/pipeline_cross_validation.svg)\n",
    "\n",
    "We'll first download data from the UCI ML data directory and then will perform classification by reading a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:23:23.040906Z",
     "start_time": "2020-11-23T15:23:22.963945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:27:07.263034Z",
     "start_time": "2020-11-23T15:27:07.247042Z"
    }
   },
   "outputs": [],
   "source": [
    "## Extracting a zip file\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/SMSSpamCollection.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data/SMSSpamCollection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:27:47.712451Z",
     "start_time": "2020-11-23T15:27:47.678456Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x98 in position 2365: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-61bd21d1b94b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/SMSSpamCollection/SMSSpamCollection'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\workspace\\lib\\encodings\\cp874.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x98 in position 2365: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open('data/SMSSpamCollection/SMSSpamCollection') as f:\n",
    "    data = [line.strip().split('\\t') for line in f.readlines()]\n",
    "\n",
    "y, text = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:20:09.784402Z",
     "start_time": "2020-11-23T15:20:09.762319Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-a5f613b72dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "collections.Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:53:52.315256Z",
     "start_time": "2020-11-23T15:53:52.169258Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-0e10a96e7ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/spam.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\workspace\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\workspace\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\workspace\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\workspace\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/spam.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
