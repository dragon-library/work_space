{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตรวจการสะกดคำภาษาไทย ด้วย PyThaiNLP\n",
    "====\n",
    "\n",
    "- [Reference : ](https://www.bualabs.com/archives/3895/what-is-spell-checker-thai-language-spell-checker-pythainlp-spelling-correction-python-pythainlp-ep-3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "จากใน ep ที่แล้ว เราได้ใช้งาน PyThaiNLP  [ตัดคำภาษาไทย](https://www.bualabs.com/archives/3740/python-word-tokenize-pythainlp-example-algorithm-deepcut-newmm-longest-python-pythainlp-ep-2/)  ตัดข้อความยาว ๆ Tokenization ออกมาเป็น Token เรียบร้อยแล้ว ใน ep นี้ เราจะมาดูว่า แต่ละ Token นั่นสะกดถูกหรือไหม Spellchecker รวมไปถึงแนะนำ และแก้ไขให้ถูกต้อง Spelling Correction ก่อนที่จะนำไปป้อนให้โมเดลในงานวิเคราะห์ทางด้าน  [NLP](https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/)  ต่อไป\n",
    "\n",
    "ในงาน  [NLP](https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/)  การใช้งานจริง เช่น การรับ Input จาก User สิ่งที่เราจะได้พบเจออยู่ตลอด คือ User กรอกข้อมูลผิด ยิ่งข้อมูลที่ไม่ใช่ข้อมูลที่มีโครงสร้าง Structure Data, ไม่มี Data Type ตัวเลข หรือวันเวลา ที่มีรูปแบบแน่นอน แต่เป็นข้อความ Free Text ที่มีคำสะกดผิดปนอยู่ ทำให้การ Validate ข้อมูล อาจจะทำได้ยาก\n",
    "\n",
    "## Spellchecker คืออะไร\n",
    "\n",
    "![Google Spell Checker กางเกน Did you mean: กางเกง](https://www.bualabs.com/wp-content/uploads/2020/02/google-spell-checker.png)\n",
    "\n",
    "Google Spell Checker กางเกน Did you mean: กางเกง\n",
    "\n",
    "Spell checker คือ โปรแกรมตรวจการสะกด ตรวจคำผิด ว่าข้อความ คำที่ User กรอกเข้ามามีปรากฎอยู่ใน Dictionary หรือไม่ โดยอาจจะแนะนำคำใกล้เคียง ที่น่าจะเป็นคำที่ถูกต้องให้ User เลือก หรือแม้กระทั่งเลือกให้โดยอัตโนมัติ เรียกว่า Spelling Correction\n",
    "\n",
    "ในเคสของภาษาไทย โดย Default แล้ว [Spellchecker](https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/) ของ PyThaiNLP จะใช้อัลกอริทึม ของ Peter Norvig ที่จะหารายการคำใกล้เคียงจาก Dictionary โดยใช้จำนวนอักษรที่ผิด 1, 2, … ตัวอักษร ผสมกับความน่าจะเป็น จากความถี่ของคำนั้นที่ปรากฎใน [Corpus](https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/)  ดัง Source Code ตัวอย่าง ภาษา Python ด้านล่าง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'big.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b47dc728148e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mWORDS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'big.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWORDS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'big.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "โดยอัลกอริทึม ของ Peter Norvig ไม่ได้ใช้ Context หรือคำแวดล้อมที่มาก่อนหน้า หรือต่อจากนั้น และไม่ได้ใช้ตำแหน่งปุ่มใกล้เคียง บน Keyboard มาคำนวนความน่าจะเป็น\n",
    "\n",
    "## NorvigSpellChecker\n",
    "\n",
    "เราสามารถเลือก กำหนด parameter ของ Dictionary ของ Spellchecker ได้ตามต้องการ ดังนี้\n",
    "\n",
    "-   custom_dict (str) – list of tuple (คำศัพท์, ความถี่) สำหรับสร้าง Dictionary ที่จะใช้ใน Spellchecker (Default คือ Thai National Corpus TNC ที่ตามเงื่อนไข Default ด้านล่าง แล้วจะมีคำศัพท์ประมาณ 40,000 คำ)\n",
    "-   min_freq (int) – ความถี่น้อยที่สุดจะเอาไว้ ใน Dictionary (default = 2)\n",
    "-   min_len (int) – ความยาวน้อยที่สุดที่จะเอาไว้ ใน Dictionary (เป็นตัวอักษร) (default = 2)\n",
    "-   max_len (int) – ความยาวสูงสุดของคำที่จะเอาไว้ ใน Dictionary (default = 40)\n",
    "-   dict_filter (func) – ฟังก์ชันที่จะกรอง Dictionary โดย Default แล้วจะลบคำที่มีตัวเลข และคำที่ไม่ใช่ภาษาไทยทิ้งไป จาก Dictionary ถ้าไม่ต้องการใช้ Filter ให้ใส่ None\n",
    "\n",
    "# Spelling Correction\n",
    "\n",
    "ส่วน Spelling Correction คือการเลือกคำที่มีความน่าจะเป็นสูงสุด ในที่นี้ คือ คำแรกของ List รายการที่ได้จาก Spellchecker ด้านบน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในงาน NLP การใช้งานจริง เช่น การรับ Input จาก User สิ่งที่เราจะได้พบเจออยู่ตลอด คือ User กรอกข้อมูลผิด ยิ่งข้อมูลที่ไม่ใช่ข้อมูลที่มีโครงสร้าง Structure Data, ไม่มี Data Type ตัวเลข หรือวันเวลา ที่มีรูปแบบแน่นอน แต่เป็นข้อความ Free Text ที่มีคำสะกดผิดปนอยู่ ทำให้การ Validate ข้อมูล อาจจะทำได้ยาก\n",
    "\n",
    "จากใน ep ที่แล้ว เราได้ใช้งาน PyThaiNLP ตัดคำภาษาไทย ตัดข้อความยาว ๆ Tokenization ออกมาเป็น Token เรียบร้อยแล้ว ใน ep นี้ เราจะมาดูว่า แต่ละ Token นั่นสะกดถูกหรือไหม Spellchecker รวมไปถึงแนะนำ และแก้ไขให้ถูกต้อง Spelling Correction ก่อนที่จะนำไปป้อนให้โมเดลในงานวิเคราะห์ทางด้าน NLP ต่อไป"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dev version\n",
    "# !pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
    "\n",
    "# release version \n",
    "#! pip install pythainlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart Runtime เพื่อให้ใช้ Library เวอร์ชัน ที่เพิ่ง Install ลงไป\n",
    "\n",
    "ในเคสนี้ เราจะปิด Warning ไว้ก่อน จะได้อ่านผลลัพท์ง่าย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pythainlp\n",
    "\n",
    "pythainlp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PyThaiNLP Spellchecker[](https://www.bualabs.com/archives/3895/what-is-spell-checker-thai-language-spell-checker-pythainlp-spelling-correction-python-pythainlp-ep-3/#2.-PyThaiNLP-Spellchecker)\n",
    "\n",
    "## Spellchecker โปรแกรมตรวจตัวสะกด[](https://www.bualabs.com/archives/3895/what-is-spell-checker-thai-language-spell-checker-pythainlp-spelling-correction-python-pythainlp-ep-3/#2.1-Spellchecker-%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B8%95%E0%B8%A3%E0%B8%A7%E0%B8%88%E0%B8%95%E0%B8%B1%E0%B8%A7%E0%B8%AA%E0%B8%B0%E0%B8%81%E0%B8%94)\n",
    "\n",
    "โดย Default แล้ว  [Spellchecker](https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/)  ของ PyThaiNLP จะใช้อัลกอริทึม ของ Peter Norvig ที่จะหารายการคำใกล้เคียงจาก Dictionary โดยใช้จำนวนอักษรที่ผิด 1, 2, ... ตัวอักษร ผสมกับความน่าจะเป็น จากความถี่ของคำนั้นที่ปรากฎใน  [Corpus](https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/)\n",
    "\n",
    "โดยไม่ได้ใช้ Context หรือคำแวดล้อมที่มาก่อนหน้า หรือต่อจากนั้น และไม่ได้ใช้ตำแหน่งปุ่มใกล้เคียง บน Keyboard มาคำนวนความน่าจะเป็น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['รั้ว', 'รั่ว', 'อั๊ว', 'รัว', 'ปั๊ว', 'จั๊ว', 'ตั๊ว', 'กั๊ว', 'ลั๊ว']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"รั๊ว\") # รัว ไม้ตรี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กว่า', 'กวาด', 'กวาง', 'กวา', 'กวาน', 'กวาว', 'กว้า']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"กวาา\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['น้ำส้ม', 'น้ำนม']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"น้ำสม\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['สาธารณสุข']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spell(\"ศาธารณสุบ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ร้อน', 'ร้อง', 'ร้อย', 'ร้อก', 'ร้อ', 'ร้อบ', 'ร้อด', 'ร้อม']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"ร้อ4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['รั้ว', 'รั้ง', 'รั้น']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"รั้o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spellchecker ด้วย Custom Dictionary และ Word Frequency\n",
    "\n",
    "โดย Default จะใช้ Dictionary จาก Thai National Corpus (TNC) ซึ่งจะมีคำศัพท์ ใน Corpus จำนวนเกือบ 4 หมื่นคำ (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.spell import NorvigSpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39964"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker = NorvigSpellChecker()  # use default filter (remove any word with number or non-Thai character)\n",
    "len(checker.dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ด่างทับทิม', 2),\n",
       " ('ภาณุมาศ', 5),\n",
       " ('อธิบ', 2),\n",
       " ('คอบ', 58),\n",
       " ('โจ้ว', 35),\n",
       " ('อาร์ลส์', 8),\n",
       " ('พวย', 24),\n",
       " ('โง้ง', 54),\n",
       " ('สีห์', 45),\n",
       " ('รียส์', 2),\n",
       " ('ยกเว้น', 3299),\n",
       " ('โฟร์แฮนด์', 5),\n",
       " ('เวลธ์', 3),\n",
       " ('ฮารา', 68),\n",
       " ('วิชาการ', 4007),\n",
       " ('บทคัดย่อ', 22),\n",
       " ('แกนนำ', 979),\n",
       " ('ฮอพ', 10),\n",
       " ('ทั่ม', 6),\n",
       " ('กตัญ', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ตัวอย่าง คำศัพท์ และ ความถี่ ที่อยู่ใน Dictionar\n",
    "\n",
    "list(checker.dictionary())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เราสามารถกำหนด Custom Dictionary เพิ่มคำที่เราต้องการได้เอง ในเคสนี้จะใช้ Thai Textbook Corpus (ttc) แทน tnc\n",
    "\n",
    "from pythainlp.corpus import ttc  # Thai Textbook Corpus\n",
    "\n",
    " # ttc??\n",
    "# NorvigSpellChecker??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['เสียง',\n",
       " 'เพียง',\n",
       " 'เรียง',\n",
       " 'เตียง',\n",
       " 'เถียง',\n",
       " 'เอียง',\n",
       " 'เคียง',\n",
       " 'เวียง',\n",
       " 'เฉียง',\n",
       " 'เจียว',\n",
       " 'เขียง',\n",
       " 'เจียม',\n",
       " 'เชียง',\n",
       " 'เจียน',\n",
       " 'เมียง',\n",
       " 'เลียง',\n",
       " 'เจียก',\n",
       " 'เจียร',\n",
       " 'เจียด',\n",
       " 'เจียงๆ',\n",
       " 'เหียง',\n",
       " 'เจีย',\n",
       " 'เกียง',\n",
       " 'เนียง']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker = NorvigSpellChecker(custom_dict=ttc.word_freqs())\n",
    "checker.spell(\"เจียง\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16377, 19493)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# นับจำนวนคำทั้งหมดใน Dictionary จาก ttc จะเห็นว่าไม่เท่ากัน เนื่องจากการสร้าง Spellchecker จะมีกำหนด Parameter เพื่อกรอง Filter คำศัพท์ ที่มีความถี่น้อยเกินไป, ความยาวน้อยเกินไป, ยาวเกินไปทิ้งไป, etc. ก่อนที่จะนำไปใช้\n",
    "\n",
    "len(checker.dictionary()), len(ttc.word_freqs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ออกเรือน', 5),\n",
       " ('ผ่อนคลาย', 30),\n",
       " ('ทำ', 12650),\n",
       " ('บาล', 17),\n",
       " ('จดหมายเหตุ', 47),\n",
       " ('สกรรมกริยา', 9),\n",
       " ('โรงพยาบาล', 229),\n",
       " ('หมิ่นประมาท', 24),\n",
       " ('มนุษยศาสตร์', 2),\n",
       " ('โศกนาฏกรรม', 7),\n",
       " ('พง', 76),\n",
       " ('หยด', 115),\n",
       " ('สโคป', 3),\n",
       " ('ขิม', 2),\n",
       " ('สำลัก', 11),\n",
       " ('อมโรค', 2),\n",
       " ('ครอง', 252),\n",
       " ('นิ่งๆ', 45),\n",
       " ('ประมวล', 45),\n",
       " ('ดึงดื้อ', 4)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ดูตัวอย่าง คำศัพท์จาก ttc ที่ filter มาแล้ว\n",
    "\n",
    "list(checker.dictionary())[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราสามารถเลือก กำหนด parameter ของ Dictionary ของ Spellchecker ได้ตามต้องการ ดังนี้\n",
    "\n",
    "NorvigSpellChecker\n",
    "\n",
    "custom_dict (str) – list of tuple (คำศัพท์, ความถี่) สำหรับสร้าง Dictionary ที่จะใช้ใน Spellchecker (Default คือ Thai National Corpus TNC ที่ตามเงื่อนไข Default ด้านล่าง แล้วจะมีคำศัพท์ประมาณ 40,000 คำ)\n",
    "\n",
    "min_freq (int) – ความถี่น้อยที่สุดจะเอาไว้ ใน Dictionary (default = 2)\n",
    "\n",
    "min_len (int) – ความยาวน้อยที่สุดที่จะเอาไว้ ใน Dictionary (เป็นตัวอักษร) (default = 2)\n",
    "\n",
    "max_len (int) – ความยาวสูงสุดของคำที่จะเอาไว้ ใน Dictionary (default = 40)\n",
    "\n",
    "dict_filter (func) – ฟังก์ชันที่จะกรอง Dictionary โดย Default แล้วจะลบคำที่มีตัวเลข และคำที่ไม่ใช่ภาษาไทยทิ้งไป จาก Dictionary ถ้าไม่ต้องการใช้ Filter ให้ใส่ None\n",
    "\n",
    "ในเคสนี้ไม่กำหนด custom_dict คือกลับไปใช้ TNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30376"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker = NorvigSpellChecker(min_freq=5, min_len=2, max_len=15)\n",
    "len(checker.dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66209"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# เอา TNC มาใช้ แบบเต็ม ๆ ไม่กรองตัวเลข และตัวอักษรภาษาอื่น ๆ\n",
    "\n",
    "checker_no_filter = NorvigSpellChecker(dict_filter=None)  # use no filter\n",
    "len(checker_no_filter.dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66207"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  ตัวอย่างการกรอง Filter ที่ซับซ้อนขึ้นกว่า ความยาว ความถี่ ด้วย dict_filter เช่น ฟังก์ชันลบคำศัพท์ที่มี อักษร ไปยาลน้อย\n",
    "\n",
    "def remove_smallpiyan(word):\n",
    "    return False if \"ฯ\" in word else True\n",
    "\n",
    "checker_custom_filter = NorvigSpellChecker(dict_filter=remove_smallpiyan)  # use custom filter\n",
    "len(checker_custom_filter.dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyThaiNLP Spelling Correction\n",
    "\n",
    "ส่วน Spelling Correction คือการเลือกคำที่มีความน่าจะเป็นสูงสุด ในที่นี้ คือ คำแรกของ List รายการที่ได้จาก Spellchecker ด้านบน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'รั้ว'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp import correct\n",
    "\n",
    "correct(\"รั๊ว\") # รัว ไม้ตรี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'กว่า'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct(\"กวาา\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'น้ำส้ม'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct(\"น้ำสม\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สาธารณสุข'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct(\"ศาธารณสุบ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the percentage of Thai characters in the textt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from pythainlp.util import countthai\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"ดอนัลด์ จอห์น ทรัมป์ English: Donald John Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.130434782608695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"ดอนัลด์ จอห์น ทรัมป์ English: Donald John Trump\",  ignore_chars=string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize ข้อความ\n",
    "\n",
    "บางทีข้อความอาจจะมีการใส่ตัวอักษรซ้ำ ๆ เบิ้ลมา เราต้องจัดการ ลบสระ วรรณยุกต์ที่ซ้ำซ้อน ทิ้งไป ดังตัวอย่าง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import normalize\n",
    "\n",
    "# normalize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"กิิิ่ง\") == \"กิ่ง\"  # กิ่ง สระ อิ 3 ตัว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"ก้าาาน\") == \"ก้าน\"  # ก้าน สระ อา 3 ตัว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"ใใบ\") == \"ใบ\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"เเปลก\") == \"แปลก\"  # เ เ ป ล ก  vs แ ป ล ก"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the percentage of Thai characters in the text \n",
    "\n",
    "- https://thainlp.org/pythainlp/docs/2.0/api/util.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import string\n",
    ">>> from pythainlp.util import countthai\n",
    ">>>\n",
    ">>> string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"ดอนัลด์ จอห์น ทรัมป์ English: Donald John Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"(English: Donald John Trump)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.130434782608695"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"ดอนัลด์ จอห์น ทรัมป์ English: Donald John Trump\", \\\n",
    "    ignore_chars=string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if all character is Thai character\n",
    "\n",
    "- https://thainlp.org/pythainlp/docs/2.0/api/util.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import isthai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isthai(\"กาลเวลา\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isthai(\"กาลเวลา,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isthai(\"กาลเวลา3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isthai(\"กาลเวลาDw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isthai(\"กาลเวลา, การเวลา-ก,  3.75$\", ignore_chars=\"1234567890.-,$ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### บทความที่เกี่ยวข้อง:\n",
    "\n",
    "1.  [PyThaiNLP คืออะไร Tutorial สอนใช้งาน PyThaiNLP Library NLP ภาษาไทย สำหรับ Python เบื้องต้น – PyThaiNLP ep.1](https://www.bualabs.com/archives/3234/what-is-pythainlp-tutorial-teach-basic-how-to-use-pythainlp-library-nlp-in-python-pythainlp-ep-1/ \"PyThaiNLP คืออะไร Tutorial สอนใช้งาน PyThaiNLP Library NLP ภาษาไทย สำหรับ Python เบื้องต้น – PyThaiNLP ep.1\")\n",
    "2.  [Latent Semantic Analysis (LSA) คืออะไร Text Classification ด้วย Singular Value Decomposition (SVD), Non-negative Matrix Factorization (NMF) – NLP ep.4](https://www.bualabs.com/archives/2971/lsa-latent-semantic-analysis-text-classification-singular-value-decomposition-svd-non-negative-matrix-factorization-nmf-nlp-ep-4/ \"Latent Semantic Analysis (LSA) คืออะไร Text Classification ด้วย Singular Value Decomposition (SVD), Non-negative Matrix Factorization (NMF) – NLP ep.4\")\n",
    "3.  [Recurrent Neural Network (RNN) คืออะไร Gated Recurrent Unit (GRU) คืออะไร สอนสร้าง RNN ถึง GRU ด้วยภาษา Python – NLP ep.9](https://www.bualabs.com/archives/3103/what-is-rnn-recurrent-neural-network-what-is-gru-gated-recurrent-unit-teach-how-to-build-rnn-gru-with-python-nlp-ep-9/ \"Recurrent Neural Network (RNN) คืออะไร Gated Recurrent Unit (GRU) คืออะไร สอนสร้าง RNN ถึง GRU ด้วยภาษา Python – NLP ep.9\")\n",
    "4.  [Sentiment Classification วิเคราะห์รีวิวหนัง IMDB แง่บวก แง่ลบ ด้วย AWD_LSTM Deep Neural Network เทรนแบบ ULMFiT Transfer Learning – NLP ep.8](https://www.bualabs.com/archives/3087/sentiment-classification-deep-learning-imdb-movie-reviews-positive-negative-deep-neural-network-awd-lstm-ulmfit-nlp-ep-8/ \"Sentiment Classification วิเคราะห์รีวิวหนัง IMDB แง่บวก แง่ลบ ด้วย AWD_LSTM Deep Neural Network เทรนแบบ ULMFiT Transfer Learning – NLP ep.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"5 Reasons Why My Girlfriend Thinks She's Not Beautiful Enough, No Matter What Anyone Tells Her\\n\", \"The Perfect Reply A Girl Can Give To The Question 'What's Your Favorite Position?'\\n\", 'When A Simple Idea Like This Actually Works And It Helps People Get By, Everybody Wins\\n', 'One Of The Biggest Lies We’re Encouraged To Tell Ourselves About Our Value To Society Is Right Here\\n', 'He Was About To Take His Own Life — Until A Man Stopped Him. Here He Meets Him Face To Face Again.\\n', \"When I Was A Kid, An Ad Aired On TV That I Didn't Fully Get. Now, I Want Us All To Watch It Again.\\n\", \"These Parents Think It Might Be A Phase, But They Just Don't Understand What It Means\\n\", 'How China Deals With Internet-Addicted Teens Is Kind Of Shocking. And Maybe A Good Idea?\\n', 'If You Want A Successful Long-Term Relationship (Of Any Kind), Here Are 3 Invaluable Things To Know\\n', 'How Do You Know If You Have Depression? Hear This Woman Explain How She Found Out.\\n', \"If You Ever Wanted The Great Novels Explained To You By Your Thug Friend, Now's Your Chance\\n\", \"Seems Like Any Other High School, Right? So What's The Major Thing Missing From These Pictures?\\n\", 'Every Person On Earth Is Supposed To Have These. But What Are They Exactly?\\n', 'Jon Stewart Delivers One Of The Best Interviews In Recent Memory\\n', 'If You Give A Bride A Beautiful Set Of Bone China, You Set Her Table For A Day\\n', 'A High School In A Poor Neighborhood Closed Down. These Folks Reopened It And Kicked Some Butt. How?\\n', 'The Super Bowl Ad That You Should See If You Think Little Girls Can Become Epic Rocket Scientists\\n', \"The NFL Would Never Let This Ad Air On The Super Bowl, So We're Gonna Show You It. It's Important.\\n\", 'The Simple Way A Developing Country Managed To Decrease The Number Of Babies Who Died By Almost 30%\\n', 'When There’s Nothing Scarier Than A Room Full Of White Men Laughing\\n', 'How Bette Midler’s Pet Peeve Made New York City A Better Place To Live\\n', 'When ‘They’ Say Cutting Taxes On The Rich Means Job Creation, They’re Lying. Just Ask This Rich Guy.\\n', \"I Wonder If Anyone That Rich Thinks, 'Yeah, I Need All This Money. All. Of. It.'\\n\", \"'I Just Wanted To Die' And 'Maybe I Deserved This' Are Words A Child Should Never Have To Say\\n\", \"Something Absolutely Terrible Just Happened To The Internet. Here's Why.\\n\", \"The Massive Corporate Power Grab You're Not Supposed To Know About\\n\", 'Why Is Relatively Normal Child-Like Behavior Being Treated As An Illness? Gue$$.\\n', 'An App That Talks Back, But Not In That Mildly Entertaining Way Siri Does\\n', 'This Is A Film I Want To See. Because Stigmas Are Deadly And Stories Need to Be Told.\\n', '11 Things To Keep In Mind Before Dating A Black Woman. The Last One Is Definitely No Joke.\\n', 'With Songs That Matter Like This, I Know Why This Country Singer Has A Grammy\\n', \"The Truth About The Warm Weather We've Actually Been Feeling This Winter\\n\", 'Here Are Some Secrets About An Industry That Nobody Wants To Talk About\\n', 'Huge Company Looks At Big Fine And Just Laughs And Laughs And Laughs\\n', 'Reporter Asks The Wrong Question, Politician Threatens To Throw Reporter Off A Balcony. Yes, Really.\\n', 'WARNING: This Type Of Dancing Can Lead To Severe Empowerment And A Strong Connection With Others\\n', \"That One Time A Company Ignored The Racist Haters And Proceeded With Another 'Shocking' Commercial\\n\", 'How Some Tweets Sparked A Movement That Launched An App That Might Just End Sexism In Advertising\\n', \"It's Time To Stop Saying 'Like A Boss.' Swedes Are Way Cooler.\\n\", \"Your Daughter May Be Rebellious, But At Least She's Not Leaving Dead Villagers All Over The House\\n\", \"30,000 People Die Each Year Getting Something They Don't Need. It's Time We Address The Problem.\\n\", \"At First You'll Be Confused By The Story In The Beginning, But Then It Hits You Like A Ton Of Bricks\\n\", 'Come For The Adorable Orangutans. Stay Because There Is Something You Should Know About Them.\\n', 'A Company Does Something Really Puzzling On A Beach, And I Think They Made Their Point\\n', 'America Has A Dirty Little Secret, And This Congressman Just Exposed It\\n', 'A Before And After That Has Me In Disbelief: This Is Drinking Water?\\n', 'A Hunger Crisis In America Is Happening, And It’s Time We Spoke Up About It\\n', '1/3 Of Women With Speaking Roles In Movies Are More Likely To Be … Naked?\\n', \"A Celebrity Gossip Idea So Brilliant I Wish It Wasn't A Parody\\n\", \"Hear How Lupita Nyong'o Only Considered Herself Black When She Came To America\\n\", 'We Don’t Usually Make Fun Of Terrorism, But When We Do, We Make A Damn Good Point About Freedom\\n', '1 Minute Of Some Numbers About Income That Feel Colder Than Any Polar Vortex\\n', 'A Mom Begged The Judge To Let The Sentence Fit The Crime. He Ignored Her And Gave Her Son Life.\\n', \"A Whole New Way To Think About Stress That Changes Everything We've Been Taught\\n\", \"Disney Introduces A Gay Couple On A Kids' Show, Confusing Children Everywhere. Wait, No...\\n\", 'When It Comes Right Down To It … After We’re Gone, It’s Just Words, Memories, And Photographs\\n', 'Not A Single Woman Has Been Invited To Join The Syrian Peace Talks. Here They Are Instead.\\n', \"The Sharp Response 'SNL' Gave Victoria's Secret After They Banned A Mom From Breastfeeding In Store\\n\", \"Do You Spot What's Not Quite Right With These Fashion Posters?\\n\", \"If I Had A Hammer, I'd Nail This Pete Seeger Interview To My Wall\\n\", 'A Nurse Uses His Special Gift To Help Patients Cope With Pain\\n', \"When Girls Are Constantly Shamed For Their Looks, Is It Any Wonder There's Only 1 Photog They Trust?\\n\", \"A Little Girl Who Isn't Allowed To Play Outside, And The Startling Reason Why\\n\", 'Here’s Pete Seeger Doing One Of The Songs He Loved So Much\\n', 'A Single Dad Gets Creative To Deal With The Lonely Moments After A Weekend With His Kids\\n', '1 Out Of 3 Women Live On The Edge Of Something Terrifying\\n', 'A Scientist Laughs In The Face Of Sexism, Totally Rocks Biology Career\\n', 'Nancy Grace Loses A Debate. Against Herself.\\n', \"They Said She Was Paralyzed, And She Couldn't Afford Her Recovery. Then A Dancer Changed It All.\\n\", 'I Sing Along To This All The Time. But When She Does, It Makes Me Really, Really Uncomfortable.\\n', 'People Have Been Driving Around Like Idiots For A Long, Long Time\\n', 'To Prove Humans Could Be Happier With Simpler Lives, She Came Up With An Elaborate Experiment\\n', 'The 3 Ridiculous Reasons Why Drugs Were Prohibited In The First Place\\n', \"What You Can Expect From Africa Isn't What Many Of Us Have Been Taught To Expect From Africa\\n\", 'These Badass Kid Skaters In Ethiopia Are Awesome\\n', \"Forget Vampires And Zombies, Here's A Real-Life Monster Way More Worthy Of Our Attention\\n\", 'The U.S. Dept. Of State Asked Maya Angelou To Speak About Him. What She Said Is Unforgettable.\\n', 'Watch A Clever Senator Smack Down Deniers With His Extraordinary Commentary\\n', 'A 13-Year-Old Explains The Religion Behind Marriage Equality. My Head Now Hurts From Nodding Along.\\n', \"Watch 33 Gay And Straight Couples Get Married On The Grammys Without Hurting Anyone Else's Marriage\\n\", 'Wanna See Something Brilliant? Take A Few Days Off, Then Look In The Mirror.\\n', 'Watch The Spread of Walmart Across The Country In One Horrifying GIF\\n', 'A Comic That Might Cause You To Realize Other Countries Contain Actual People\\n', 'In What World Is It OK To Open A Bank Account In A Stranger’s Name? Oh Yeah, This One.\\n', \"Lady Friends, Here's A Simple Question You Shouldn't Have To Ask Your Doctor\\n\", 'A World Gone Mad (For Cows)!? Bad Beef Bacteria And Cow Poo Climate Change.\\n', \"Here's Your Chance: Meet The Women Who Make Clothes Like Yours Day In And Day Out\\n\", \"He's Trying To Make Buses Sexy, And It's Working\\n\", 'Lady Stereotypes, Right After Your Commercial Break Full Of Female Stereotypes\\n', 'An Eye-Opener Of A History Lesson On What Republican Presidents Thought About Gun Control\\n', 'Forget Obamacare — What’s Really Wrong With Our Medical System Is Right Here\\n', \"I Was In A Lot Of Clubs In High School. I'm Pretty Sure None Of Them Were As Cool As This!\\n\", 'D’ya Ever Think These Guys Are Just Doing It For The Attention?\\n', 'The Governor Is Proud Of His Budget. This Student Wants To Teach Him A Lesson.\\n', \"She Has Experienced So Much Hate Because Of Who She Married. You'd Think He Was A Monster.\\n\", \"A Straight Reporter Says, 'I'd Be Offended By You If I Were Gay.' This Actor's Response? Epic.\\n\", \"If You Needed A Better Reason To Start Cooking More Meals At Home, Here's One For You\\n\", \"You Don't Care About Your Privacy, And Kim Kardashian Proves It\\n\", \"This Guy Needs A Clue: A Member Of The 1% Declares It 'Great' That 3.5 Billion Are In Poverty.\\n\", \"'Don't Ask, Don't Tell' Ended In 2010, But It's An ID Card That Makes It Real For These Families\\n\", \"Here's The Moment A Black Woman Protected A White Man At A KKK Rally\\n\", \"Here's A Fact About Sharks And Humans That'll Stay With You Every Hour On The Hour Today\\n\", '163 Years Ago, A Former Slave Rocked The World With These Words\\n', 'This Filmmaker Is Showing Hollywood What A Real Leading Lady Looks Like. Glad We + You Could Help.\\n', 'An Avenger Talks About The Hell His Mom Went Through Back When Women Had No Choices\\n', \"Sexual Objectification: What It Is, Why It's Damaging, And How We Change It\\n\", 'All The Science Reasons Redheads Do That Redhead Thing They Redhead Do So Well\\n', 'A Video I Loved Within 10 Seconds From A Random Dude Explaining The Magic Of Birth Control\\n', 'Raised By Black Panthers. Made History With Music. Died At 25. But Hear Him In His Own Words.\\n', 'There Are People In Our Lives Who Are So Good At What They Do, You Never See Them. Here’s One Story.\\n', \"WATCH: This Is One Time-Lapse Big Oil Doesn't Want You To See\\n\", 'A News Team Follows Potential Models For One Week. My Face Is Now Stuck In Disgust Mode.\\n', \"A 13-Year-Old Gets Plastic Surgery To Fix What's Wrong With Her. Society Is What's Wrong With Her.\\n\", 'The Authorities In"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 59803 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = \"data/upworthy_titles.txt\"\n",
    "try:\n",
    "    upworthy_titles = open(paths, encoding = 'utf-8')\n",
    "    print(upworthy_titles.readlines())\n",
    "   # perform file operations\n",
    "finally:\n",
    "       upworthy_titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'splitlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ec18a1ac2142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mupworthy_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupworthy_titles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupworthy_titles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mupworthy_titles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'splitlines'"
     ]
    }
   ],
   "source": [
    "upworthy_titles = upworthy_titles.splitlines()\n",
    "\n",
    "print (len(upworthy_titles))\n",
    "\n",
    "upworthy_titles[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x99 in position 293: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-39d2184cee89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mupworthy_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/upworthy_titles.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mupworthy_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupworthy_titles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp874.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x99 in position 293: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "urls = \"data/upworthy_titles.txt\"\n",
    "\n",
    "upworthy_titles = open(urls, \"r\")  \n",
    " \n",
    "\n",
    "\n",
    "upworthy_titles = open('data/upworthy_titles.txt', \"r\").read()\n",
    "\n",
    "upworthy_titles = upworthy_titles.decode('utf-8')\n",
    "upworthy_titles = upworthy_titles.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x99 in position 293: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f68dfb920c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/upworthy_titles.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r+\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Reading form a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp874.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x99 in position 293: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open(\"data/upworthy_titles.txt\", \"r+\") as file1: \n",
    "    # Reading form a file \n",
    "    print(file1.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upworthy_titles.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-29-ca2aa0c0c5b7>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-ca2aa0c0c5b7>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    for line in fp:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    filepath = \"data/upworthy_titles.txt\"\n",
    "\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
    "        sys.exit()\n",
    "  \n",
    "    bag_of_words = {}\n",
    "    with open(filepath) as fp:\n",
    "           cnt = 0\n",
    "            for line in fp:\n",
    "                print(\"line {} contents {}\".format(cnt, line))\n",
    "                record_word_cnt(line.strip().split(' '), bag_of_words)\n",
    "                cnt += 1\n",
    "                sorted_words = order_bag_of_words(bag_of_words, desc=True)\n",
    "    print(\"Most frequent 10 words {}\".format(sorted_words[:10]))\n",
    "     \n",
    "def order_bag_of_words(bag_of_words, desc=False):\n",
    "            words = [(word, cnt) for word, cnt in bag_of_words.items()]\n",
    "            return sorted(words, key=lambda x: x[1], reverse=desc)\n",
    "\n",
    "def record_word_cnt(words, bag_of_words):\n",
    "    for word in words:\n",
    "        if word != '':\n",
    "            if word.lower() in bag_of_words:\n",
    "                bag_of_words[word.lower()] += 1\n",
    "            else:\n",
    "                bag_of_words[word.lower()] = 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245.27px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
